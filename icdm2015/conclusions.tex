In this paper we propose an information theoretic approach for measuring diversity in text. At the heart of this approach lies a word distributional model and we present a suitable word-to-topic representation based on an LDA learned over a corpus of documents. We also present a set of enhancements to the base model to account for word importance and sample biases. Our results in two different real world domains show how this method outperforms the previously established  diversity measures in an unsupervised setting. In supervised setting we also show that our proposed approach gives a higher precision and a marginally higher accuracy compared to the baselines.

There are many text domains, where finding interesting items is valuable, 
like, for instance, news articles. Moreover, we believe that Jensen-Shannon 
divergence could be applied in other fields in context of diversity, for example, 
to analyze multi-population systems in biology. Finally, a more thorough comparative analysis 
is needed to determine the advantages and disadvantages of distributional text 
representations derived from the Reader's model in common NLP tasks, 
against other word and text embeddings.

 