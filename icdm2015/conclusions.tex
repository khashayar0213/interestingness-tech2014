In this paper we propose an information theoretic approach for
measuring diversity in text, and show that it can be an effective way
of discovering interesting products for recommendation. At the heart
of this approach lies a 
topic-distributional model for words, that is based on an LDA topic
model learned over a corpus of documents. We present a set of
enhancements to the 
basic word representations, accounting for sample
biases, topic correlations and context-dependence. Using this framework,
we show that Jensen-Shannon Divergence is an accurate measure of text
diversity. We provide experimental results in two different real 
world domains, for which this method outperforms the previously
established diversity measures in an unsupervised setting. In
supervised setting we also show that our proposed approach gives a
higher precision and a marginally higher accuracy compared to the
baselines.

There are many text domains, where finding interesting items is valuable, 
like, for instance, news articles. However, this task still lacks a
proper formulation due to the vague nature of the term {\em
interesting}, and in some cases measuring text diversity may not be
the right equivalent. We believe, that this is an important area for
future research. On the other hand, measuring diversity has a
wide range of applications, and, we think that Jensen-Shannon 
divergence should be further studied in the context of other
fields, for example, to analyze multi-population systems in
biology. Finally, a more thorough comparative analysis  
is needed to determine the advantages and disadvantages of
distributional text representations derived from our model in common
NLP tasks, against other word and text embeddings.

 