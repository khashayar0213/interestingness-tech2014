In this paper we propose an information theoretic approach for
measuring diversity in text. At the heart of this approach lies a
topic-distributional model for words, that is based on LDA learned
over a corpus of documents. We present a set of enhancements to the
base model for distributional word representations to account for sample
biases, topic similarity and context-dependence. Moreover, we
show Jensen-Shannon Divergence to be a valid measure of text
diversity. Our results in two different real 
world domains show how this method outperforms the previously
established diversity measures in an unsupervised setting. In
supervised setting we also show that our proposed approach gives a
higher precision and a marginally higher accuracy compared to the
baselines.  

There are many text domains, where finding interesting items is valuable, 
like, for instance, news articles. Moreover, we believe that Jensen-Shannon 
divergence could be applied in other fields in context of diversity, for example, 
to analyze multi-population systems in biology. Finally, a more thorough comparative analysis 
is needed to determine the advantages and disadvantages of distributional text 
representations derived from our model in common NLP tasks, 
against other word and text embeddings.

 