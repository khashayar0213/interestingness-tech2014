In this paper we proposed an information theoretic approach for measuring diversity in text. At the heart of this approach lies a word distributional model and we presented a suitable word-to-topic representation based on an LDA learned over a corpus of documents. We also presented a set of enhancements to the base model to account for word importance and sample biases. Our results in two different real world domains show how this method outperforms the previously established  diversity measures in an unsupervised setting. In supervised setting we also showed that our proposed approach shows a higher precision and a marginally higher accuracy compared to the baselines.
