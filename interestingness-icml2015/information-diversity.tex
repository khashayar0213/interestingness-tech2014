\subsection{Distributional Representations}
\label{sec:distributional-representations}
{\bf Explain a general setting, where we have a large set of word
  occurrences. Each occurrence is assigned a label from a set of
  labels. The unique words give a partitioning of occurrences into groups.}
We assume a distributional representation for the words in the vocabulary (for a brief review
see~\cite{Turian10wordrepresentations}). A distributional representation over a vocabulary $V$ maps a word in the vocabulary to a 
probability distribution over a fixed set of contexts $C$. Often we start by a co-occurrence matrix $\cM_{|V|\times|C|}$ where each row represents a co-occurrence of a word with the set of contexts
$C$. For example if we chose C=V then we obtain the familiar {\sl word-to-word} co-occurrence representation which counts the number
of times two words co-occurred in a document corpus. Another choice is to use the set of topics learned over a document
corpus by {\sl Latent Dirichlet Allocation} (LDA)~\cite{Blei:2003:LDA:944919.944937} as the context. The rows of $\cM_{|V|\times|C|}$
are then normalized in order to obtain a probability distribution over the context. We will use the notation $P_w$ to represent
the probability distribution over the context  given a word $w$. We also use the notation $W=\{w_1,...,w_k\}$ for a bag of word representation of a text snippet. Given a word distributional representation, $\cP_{W}=\{P_{w_1},...,P_{w_k}\}$ denotes the set of probability distributions over the context for the words
in the text snippet.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Jensen-Shannon Diversity}
\label{sec:jensen-shannon-divergence}
{\bf From Entropy to Mutual Information, which is the same as JS
  Divergence. }
Notice, that calculating the entropy of the label
distribution $Q$ of the atomic elements, we seem to be ignoring the
group structure, that was introduced. To understand this further, we
will show a different interpretation of entropy that is applicable
here. Suppose that we draw uniformly a random atomic element from
$S$. Let $X$ denote a random variable denoting the group it belongs
to, and let $Y$ denote the label assigned to the group. We can now
see that diversity is equal to the mutual information $I(X;Y)$ between
those variables: 
\[I(X;Y)=H(Y)-H(Y|X) = H(Y)=H(Q),\]
which is a consequence of the fact that all atomic elements in one
group have the same label assignment, so $H(Y|X)=0$. How would this change if we consider a collection of
groups where we allow different label assignments for elements within
each group. What is the diversity of such a collection of groups? A
logical extension of the previous setting would be to use the mutual
information $I(X;Y)$ directly, and now it is no longer equal to entropy
$H(Q)$. So, we obtained the following definition:

\bed\label{mutual}
Suppose we have a set $S$ of elements divided into a collection $G$ of
groups, by a function $g:S\rightarrow G$. Additionally, each element
has a label assigned with $f:S\rightarrow T$. We define diversity
of the collection $G$ as
mutual information $I(X;Y)$ between group assignment $X$ and label
assignment $Y$, for 
a sample element drawn uniformly from $S$.
\eed
Notice the exchangeable roles of groups and labels in this
definition, which are also reflected in the symmetric nature of mutual
information. Using a well known formula for mutual information, we 
can also re-write the last definition, applying it to mixture
distributions:
\bed\label{mixture}
Let $M=\Sigma d_i P_i$ be a mixture distribution. We will define
Jensen-Shannon diversity of mixture $M$ as:
\[D_{JS}(\Sigma d_iP_i) = \sum_{i=1}^k d_i D_{KL}(P_i\|M)\]
\eed
The name refers to the Jensen-Shannon divergence measure, which
is equivalent to mutual information.

{\bf A simple generalization theorem (to be fixed)}
\bep\label{entropy}
Let $S=\{P_{w_1},...,P_{w_k}\}$ be a set of distributions over topics,
such that each has a singleton support. Assume that the prior
topic distribution $P$ corresponding to $S$ is a uniform
distribution. Then, 
\[D_S=H(P_S),\]
where $P_S$ corresponds to the overall topic distribution in the set
$S$, and $H$ denotes Shannon entropy.
\eep

{\bf When is do you get maximum diversity? When the distributions are
  evenly pushed into the corners of the simplex.}

% Given a distributional representation over a vocabulary $V$ and context $C$ with $P_w$ giving a probability distribution of a
% word $w$ over the context $C$, we can measure information diversity for a text snippet $W=\{w_1,...,w_k\}$ and its
% distributional representation $\cP_{W}=\{P_{w_1},...,P_{w_k}\}$ as follows:
% \bed\label{importance}
% Given a distribution $P_w$, its {\sl importance} with respect to a
% prior distribution $P$ is defined as $D_w = D_{KL}(P_w\|P)$ where $D_{KL}(\|)$ denotes the
% {\sl Kullback-Leibler} Divergence.
% \eed
% \bed\label{mixture}
% Given a set of distributions $\cP_{W}=\{P_{w_1},...,P_{w_k}\}$ and a
% prior $P$, we
% define a mixture distribution $P_W=\sum_{i=1}^k d_{w_i} P_{w_i}$ where $d_{w_i}=\frac{D_{w_i}}{\sum D_{w_j}}$ are the normalized
% importances.
% \eed
% Essentially, $P_W$ is the weighted average of the set $\cP_{W}$, where
% the weights are chosen according to the importances. Next we
% define the diversity measure:
% \bed\label{diversity}
% We define the Jensen-Shannon Information Diversity of a set of
% distributions $\cP_W$ with respect to 
% prior $P$ as $\mbox{JSD}_P(\cP_W)=\sum_{i=1}^k d_{w_i}D_{KL}(P_{w_i}\|P_W)$
% where $d_{w_i}$ and $P_W$ are as in the previous definition.
% \eed
% This definition is closely related to the 
% {\em general Jensen-Shannon Divergence}~\cite{FugledeTopsoe}. Another
% interesting theoretical property of Jensen-Shannon Information
% Diversity is that it can be interpreted as a generalization of Shannon
% entropy as a population diversity measure, however we will not go
% into this here any further. 

