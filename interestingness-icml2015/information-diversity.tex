\subsection{Distributional Representations}
\label{sec:distributional-representations}
Analyzing textual data often leads to a common general framework,
where we obtain a significant collection of text with each word
occurrence tagged with one (or several) among a set of
labels. Moreover, naturally, every word occurrence can be mapped to a
vocabulary term (barring ambiguities). So, each word in the vocabulary
corresponds to a group of its labeled occurrences, which leads to a
distribution over the set of labels. A distributional representation
over a vocabulary $V$ maps a word in the vocabulary to a  
probability distribution over a fixed set of labels $T$  (for a brief review
see~\cite{Turian10wordrepresentations}). Often we start by a
co-occurrence matrix $\cM_{|V|\times|T|}$ where each row represents a
co-occurrence of a word with the labels.
For example if we chose $T=V$ then we obtain the familiar {\sl
  word-to-word} co-occurrence representation which counts the number 
of times two words co-occurred in a document corpus. Another choice is
to use the set of topics learned over a document 
corpus by {\sl Latent Dirichlet Allocation}
(LDA)~\cite{Blei:2003:LDA:944919.944937} as the context. The rows of
$\cM_{|V|\times|T|}$ 
can then normalized in order to obtain a probability distribution over
the labels.
%  We will use the notation $P_w$ to represent 
% the probability distribution over the context  given a word $w$. We
% also use the notation $W=\{w_1,...,w_k\}$ for a bag of word
% representation of a text snippet. Given a word distributional
% representation, $\cP_{W}=\{P_{w_ 1},...,P_{w_k}\}$ denotes the set
% of probability distributions over the context for the words 
% in the text snippet.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Jensen-Shannon Diversity}
\label{sec:jensen-shannon-divergence}
We pose the following question: what is the right model of diversity
in text, with respect to the above framework? Notice, however, that we
need not restrict ourselves to text, because the way our
distributional model is formulated can be applied to any set of
labeled elements partitioned into groups (each group corresponding to
a vocabulary word). Suppose we have a set $S$ of elements divided into
a collection $G$ of groups, by a function $g:S\rightarrow
G$. Additionally, each element has a label assigned with
$f:S\rightarrow T$. What is the diversity between the groups? Our
starting point is the notion of Shannon 
entropy, generally considered as a well-founded measure of diversity 
for a collection of labeled elements represented by a
distribution. Here, the collection consists of groups, so it provides
finer information about label assignments. Assume for a moment, that
every element in a group has the same label - in which case we can
uniquely label each group itself. Now, we can naturally obtain an
assignment distribution for the groups, weighed by their size, and
measure its entropy. Consider a uniformly sampled element $s\in
S$. Let $X$ and $Y$ denote the random variables correspoding to the
group $g(s)$ and label $f(s)$, respectively. If each group is uniquely
labeled, then $H(Y|X)=0$, so entropic diversity of the
labeling, $H(Y)$, will be equal to the mutual information between $X$
and $Y$: 
\[I(X;Y) = H(Y) - H(Y|X) =H(Y).\]
If we break the simplifying assumption, $H(Y)$ is no longer a
reasonable metric, because if all of the groups contain uniformly
labeled elements, then the distributional representations are
indistinguishable, so there is no diversity between them, but $H(Y)$
in fact reaches its maximum value. However, as we will see, mutual
information $I(X;Y)$ provides a proper generalization of diversity. We
can formulate mutual information in terms of the distributional
representations of the groups as follows:
\bed\label{jsd-definition}
Let $P(Y)=\Sigma d_g P_g$ be the distribution of labels,
described as a mixture of distributional group representations
$P_g=P(Y|X=g)$, weighed by $d_g=P(X=g)$. Mutual information between
$X$ and $Y$ can now be derived as
\[ I(X;Y)= \sum_{g\in G} d_g D_{KL}(P_g\|M),\]
where $D_{KL}$ is the Kullback-Leibler divergence.
We will call it the {\bf Jensen-Shannon diversity} of that
mixture, and denote by $D_{JS}(\Sigma d_gP_g)$.
\eed

The name refers to the generalized Jensen-Shannon divergence measure, 
discussed in \cite{FugledeTopsoe}. The above derivation follows easily
from the corresponding definitions of KL-divergence and mutual
information. Note, that mutual information is symmetric with respect
to $X$ and $Y$, even though their roles in our model are not. This
suggests certain duality between labels and groups. Let us examine
some other basic properties of this measure.

\bep\label{jsd-properties}
Let $M=\Sigma d_g P_g$ be a mixture distribution. Jensen-Shannon
diversity of $M$ satisfies the following properties:
 \begin{enumerate}
   \item $D_{JS}(\Sigma d_gP_g)\geq 0$ with equality when all
     distributions $P_g$ are equal.
   \item $D_{JS}(\Sigma d_gP_g)\leq H(M)$ with equality when all
     $P_g$ have singleton support (i.e. $P(t)=1$ for some $t\in T$).
 \end{enumerate}
\eep
The second property tells us that maximizing this measure means
pushing all distributions $P_g$ to the corners of the simplex and
therefore minimizing their uncertainty.
From this (and the earlier discussion), it follows that Jensen-Shannon
diversity can be seen as a direct generalization of entropic
diversity, when replacing fixed label assignments 
%for elements in $G$ 
with distributed ones. 
Another known interpretation of Jensen-Shannon Diversity from
statistical point of view is that it is the expected KL-divergence
between the posterior distribution $P(Y|X)$ and the prior distribution
$P(Y)$. It represents the average number of bits of information
learned about the label of an element from knowing its group.