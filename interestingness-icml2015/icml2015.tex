%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2015stylefiles/icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}

\input{../defs}

\newtheorem{theoreme}{Theorem} %[section]
\newtheorem{proposition}[theoreme]{Proposition}
\newtheorem{lemma}[theoreme]{Lemma}
\newtheorem{definition}[theoreme]{Definition}
\newtheorem{corollary}[theoreme]{Corollary}
\newtheorem{remark}[theoreme]{Remark}
\newtheorem{example}[theoreme]{Example}
\newtheorem{examples}[theoreme]{Examples}
%\newtheorem{conjecture}[theoreme]{Conjecture}
\newtheorem{conjecture}{Conjecture}




% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{An Information Theoretic Approach to Quantifying Text Interestingness}

\begin{document} 

\twocolumn[
\icmltitle{An Information Theoretic Approach to Quantifying Text Interestingness}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}

We study the problem of automatic prediction of text interestingness and present an information theoretic approach for
quantifying it in terms of topic diversity. Our hypothesis is, in many text domains, often an interesting
concept is generated by mixing a diverse set of topics. Given a word distributional model, we present
an approach that leverages {\sl Jensen-Shannon} divergence for measuring text diversity and demonstrate how such a measure
correlates with text interestingness. We describe several different base-line algorithms and present results over two different
data sets: a collection of e-commerce products from {\sl eBay}, and a corpus of {\sl NSF} proposals.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

With the rapid growth of e-commerce, new products are increasingly populated into the market place on daily basis.  A larger subset of these products consists of our daily needs or off-the-shelf products, while a much smaller subset can be attributed as {\em unique}, {\em creative}, {\em serendipitous}, or {\em interesting} (see Figure~\ref{fig:ebay-products}). This class of products often provoke an emotive response in users and create a more engaging experience for the them (see  {\em Pinterest} for example). Automatic discovery of this type of products is an important problem in e-commerce for creating an engaging experience for the users.   Quantifying interestingness, however,  is a challenging problem. There has been considerable research on visual interestingness and aesthetic quality of images~\cite{Datta:2006:SAP:2129560.2129588,Ke:2006:DHF:1153170.1153495,IsolaParikhTorralbaOliva2011,dhar:2011,reinecke2013predicting,journals/pami/WeinshallZHKOABGNPHP12}. 
In text domain, researchers have studied different dimensions of this
problem in terms of {\em humor
  identification}~\cite{Mihalcea:2005:MCL:1220575.1220642,Davidov:2010:SRS:1870568.1870582,Kiddon11,labutov-lipson:2012:ACL2012short},
{\em text
  aesthetics}~\cite{journals:tamd:Schmidhuber10,N13-1118,ganguly:2014},
and {\em document diversity}~\cite{bache:2013}.  In this paper we only
focus on text. Our hypothesis is, many interesting texts often
present diversity in the text describing them. In examples shown in Figure \ref{fig:ebay-products}, we have highlighted words that offer the largest diversity in each case.
For example in Figure~\ref{fig:eyeshadow-iphone-case}, in the context of iPhone cases, one would expect less to observe topics that relate to makeup. In this paper we present an information-theoretic approach for measuring topic diversity based on {\em Jensen-Shannon Information Diversity} and show how it correlates with text interestingness. Measuring topic diversity in text has been previously studied by~\cite{bache:2013}. We show how our method differs from this approach and present empirical results over two different data sets: a collection of products from {\sl eBay}, and a corpus of {\sl NSF} proposals. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related-work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Information Diversity}
\label{sec:Information Diversity}
\subsection{Distributional Representations}
\label{sec:distributional-representations}
{\bf Explain a general setting, where we have a large set of word
  occurrences. Each occurrence is assigned a label from a set of
  labels. The unique words give a partitioning of occurrences into groups.}
We assume a distributional representation for the words in the vocabulary (for a brief review
see~\cite{Turian10wordrepresentations}). A distributional representation over a vocabulary $V$ maps a word in the vocabulary to a 
probability distribution over a fixed set of contexts $C$. Often we start by a co-occurrence matrix $\cM_{|V|\times|C|}$ where each row represents a co-occurrence of a word with the set of contexts
$C$. For example if we chose C=V then we obtain the familiar {\sl word-to-word} co-occurrence representation which counts the number
of times two words co-occurred in a document corpus. Another choice is to use the set of topics learned over a document
corpus by {\sl Latent Dirichlet Allocation} (LDA)~\cite{Blei:2003:LDA:944919.944937} as the context. The rows of $\cM_{|V|\times|C|}$
are then normalized in order to obtain a probability distribution over the context. We will use the notation $P_w$ to represent
the probability distribution over the context  given a word $w$. We also use the notation $W=\{w_1,...,w_k\}$ for a bag of word representation of a text snippet. Given a word distributional representation, $\cP_{W}=\{P_{w_1},...,P_{w_k}\}$ denotes the set of probability distributions over the context for the words
in the text snippet.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Jensen-Shannon Diversity}
\label{sec:jensen-shannon-divergence}
{\bf From Entropy to Mutual Information, which is the same as JS
  Divergence. }
Notice, that calculating the entropy of the label
distribution $Q$ of the atomic elements, we seem to be ignoring the
group structure, that was introduced. To understand this further, we
will show a different interpretation of entropy that is applicable
here. Suppose that we draw uniformly a random atomic element from
$S$. Let $X$ denote a random variable denoting the group it belongs
to, and let $Y$ denote the label assigned to the group. We can now
see that diversity is equal to the mutual information $I(X;Y)$ between
those variables: 
\[I(X;Y)=H(Y)-H(Y|X) = H(Y)=H(Q),\]
which is a consequence of the fact that all atomic elements in one
group have the same label assignment, so $H(Y|X)=0$. How would this change if we consider a collection of
groups where we allow different label assignments for elements within
each group. What is the diversity of such a collection of groups? A
logical extension of the previous setting would be to use the mutual
information $I(X;Y)$ directly, and now it is no longer equal to entropy
$H(Q)$. So, we obtained the following definition:

\bed\label{mutual}
Suppose we have a set $S$ of elements divided into a collection $G$ of
groups, by a function $g:S\rightarrow G$. Additionally, each element
has a label assigned with $f:S\rightarrow T$. We define diversity
of the collection $G$ as
mutual information $I(X;Y)$ between group assignment $X$ and label
assignment $Y$, for 
a sample element drawn uniformly from $S$.
\eed
Notice the exchangeable roles of groups and labels in this
definition, which are also reflected in the symmetric nature of mutual
information. Using a well known formula for mutual information, we 
can also re-write the last definition, applying it to mixture
distributions:
\bed\label{mixture}
Let $M=\Sigma d_i P_i$ be a mixture distribution. We will define
Jensen-Shannon diversity of mixture $M$ as:
\[D_{JS}(\Sigma d_iP_i) = \sum_{i=1}^k d_i D_{KL}(P_i\|M)\]
\eed
The name refers to the Jensen-Shannon divergence measure, which
is equivalent to mutual information.

{\bf A simple generalization theorem (to be fixed)}
\bep\label{entropy}
Let $S=\{P_{w_1},...,P_{w_k}\}$ be a set of distributions over topics,
such that each has a singleton support. Assume that the prior
topic distribution $P$ corresponding to $S$ is a uniform
distribution. Then, 
\[D_S=H(P_S),\]
where $P_S$ corresponds to the overall topic distribution in the set
$S$, and $H$ denotes Shannon entropy.
\eep

{\bf When is do you get maximum diversity? When the distributions are
  evenly pushed into the corners of the simplex.}

% Given a distributional representation over a vocabulary $V$ and context $C$ with $P_w$ giving a probability distribution of a
% word $w$ over the context $C$, we can measure information diversity for a text snippet $W=\{w_1,...,w_k\}$ and its
% distributional representation $\cP_{W}=\{P_{w_1},...,P_{w_k}\}$ as follows:
% \bed\label{importance}
% Given a distribution $P_w$, its {\sl importance} with respect to a
% prior distribution $P$ is defined as $D_w = D_{KL}(P_w\|P)$ where $D_{KL}(\|)$ denotes the
% {\sl Kullback-Leibler} Divergence.
% \eed
% \bed\label{mixture}
% Given a set of distributions $\cP_{W}=\{P_{w_1},...,P_{w_k}\}$ and a
% prior $P$, we
% define a mixture distribution $P_W=\sum_{i=1}^k d_{w_i} P_{w_i}$ where $d_{w_i}=\frac{D_{w_i}}{\sum D_{w_j}}$ are the normalized
% importances.
% \eed
% Essentially, $P_W$ is the weighted average of the set $\cP_{W}$, where
% the weights are chosen according to the importances. Next we
% define the diversity measure:
% \bed\label{diversity}
% We define the Jensen-Shannon Information Diversity of a set of
% distributions $\cP_W$ with respect to 
% prior $P$ as $\mbox{JSD}_P(\cP_W)=\sum_{i=1}^k d_{w_i}D_{KL}(P_{w_i}\|P_W)$
% where $d_{w_i}$ and $P_W$ are as in the previous definition.
% \eed
% This definition is closely related to the 
% {\em general Jensen-Shannon Divergence}~\cite{FugledeTopsoe}. Another
% interesting theoretical property of Jensen-Shannon Information
% Diversity is that it can be interpreted as a generalization of Shannon
% entropy as a population diversity measure, however we will not go
% into this here any further. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{The Reader's Model}
\label{sec:the-readers-model}

The above generative model matches well with a standard framework for
text analysis, where we map a text dataset onto a matrix of
word-context co-occurrences. Specifically, we consider a matrix $\Mb$,
with cell $m_{i,j}$ containing the number of times that the word $w_i\in V$ 
occurred in context of, or was assigned to, $t_j\in T$. The context may
in this case correspond to a document, another word, or a topic
from a trained LDA topic model. In fact, this last example will be our main
focus here, since this approach works best when the size of the
context set is not too large (we will address this issue later). The
analogy goes as follows: each word in our vocabulary 
represents a group of its own occurrences in the data. The labels are
the trained LDA topics. The universe
is of course the set of all word occurrences in our data - each
labeled with a vocabulary term and a topic. We imagine an entity
called Reader, that has observed all of this data, trying to learn the
language. First, the Reader computes a word distribution $P_t$ for each
topic $t$, simply as the normalized columns of matrix $\Mb$, and also
the overall topic distribution $P$ across the entire dataset of 
co-occurrences. This information represents a generative model of the
data, as in the previous section. Next, the question is what should be
the Reader's topic distribution conditioned on a certain word? This
essentially corresponds to a row of matrix $\Mb$. However, we propose
a more flexible definition. First, it uses Bayesian prior
information that the Reader might have, to address the issue of words
that are underrepresented in the data. Second, it considers that an
interpretation of a word may vary with context.

\bed
Given a set of topic word distributions $\{P_t\}_{t\in T}$, a context topic
distribution $P_c$, and a prior topic distribution $\widehat{P}$ for a word $w_i$,
we define the posterior topic distribution for $w_i$ as
\[P(t|w_i,\widehat{P},P_c) \propto P_t(w_i)\cdot P_c(t) + \alpha
\widehat{P}(t),\]
where $\alpha$ is the parameter that specifies the strength of the prior.
\eed
This definition has a nice interpretation in our model: it corresponds
to regenerating our data using $P_c$ in place of $P$ as the
universe distribution, and computing a MAP topic distribution
conditioned on $w_i$, with a Dirichlet prior concentrated around
$\widehat{P}$. The most natural application of this is to simply set
both $P_c$ and $\widehat{P}$ equal to $P$. Then, $P(t|w_i,P,P)$ is the
$i$-th row of matrix $\Mb$ normalized, and with added Bayesian
smoothing. 

The purpose of this model is to provide useful stochastic
word representations, that could be used in various NLP
tasks. However, in accordance with the topic of this discussion, we will
focus on the task of measuring diversity of a given sequence of words
$S=(w_1,...,w_k)$. We can think of it as a series of word occurrences for
which only the vocabulary label was given (and not the topic
label). Assuming no prior context, we can once again use
$P_i(\,\cdot\,)=P(\,\cdot\,|w_i,P,P)$  as the topic distribution
obtained as a result 
of observing word $w_i$. We can measure how much information is gained
in this observation using KL-divergence, obtaining
$D_{KL}(P_i\|P)$. Moreover, we can bring in some information about
relations between words, by computing $P_{ij}(\,\cdot\,) =
P(\,\cdot\,|w_i,P_i,P_j)$, 
which describes the topic distribution obtained after observing word
$w_i$ in the context of word $w_j$ (since they were both in
$S$). Finally, we define a new series of topic
distributions for each word, which are based on contexts from all other
words in the sequence, weighed by their information gain.

{\bf Explain why we use importances instead of frequencies, as the
  default model would suggest. The importance is ``How many
  occurrcences does is take to learn the word distribution with
  context as the prior''} 

\bed
Given a word $w_i$, its {\bf importance} with respect to a
prior distribution $P$ is defined as 
\[D_i = D_{KL}(P_i\|P).\]
\eed

\bed
Given a sequence $S=(w_1,...,w_k)$, we define the stochastic
representation of word $w_i$ in $S$ by 
\[\widetilde{P}_i(t) = P(t|w_i,P_i,P_i^c),\]
where
\[P_i^c = \frac{\sum_{j\neq i} D_jP_j}{\sum_{j\neq i}D_j}.\]
\eed
We can now define the notion of diversity here, which is very similar
to the one proposed in Definition \ref{jspd}.

\bed
Given a co-occurrence matrix $\Mb$, we define the Jensen-Shannon
Information Diversity of a sequence 
$S=(w_1,...,w_k)$ as
\[\mbox{JSID}_{\Mb}(S)=D_{JS}(\Sigma d_i \widetilde{P}_i),\]
where 
\[d_{i}=\frac{D_{KL}(\widetilde{P}_i\|P_i^c)}{\sum D_{KL}(\widetilde{P}_i\|P_i^c)}.\]
\eed

Notice, that we use different weights than were proposed for the
population diversity (we use surprise instead of perplexity). To
best observe the difference, let us see what happens when a given word
provides no information gain (the divergence from context distribution
is zero). In the above definition, we would put no weight on that
word, since it is irrelevant to the meaning of sequence $S$. On the
other hand, using perplexity means that all weights have to be
non-zero, which makes more sense in the population
setting. Unfortunately, the
use of different weights for the information diversity prevents us
from obtaining the generalization from Proposition
\ref{generalization}. However, it is easy to see that JSID is still a
generalization of Shannon entropy in a more narrow sense, because it
can be interpreted as mutual information. 

{\bf Discuss the benefits of doing the context stuff and the smoothing}

{\bf (4) Sample size bias problem:}  note that 
when we normalize the rows of the matrix $\cM$ to obtain a word-to-topic distributional model, the normalization factor 
for every word is simply the word count. Thus for a word $w$ which occurs very rarely
in the entire corpus $\cC$, the topic distribution will be artificially skewed and is inaccurate simply because we do not have enough data points to estimate its true word-to-topic distribution.
Now, if for this corpus it happens that the prior topic distribution
is close to uniform, it will give a very high importance to the word
$w$ when measuring the information diversity as described in
Section~\ref{sec:information-diversity}.
One way to alleviate this
problem is to use the relative sample size (e.g., word count) to
smooth the distribution obtained by normalizing the rows of matrix
$\cM$. A natural choice for the smoothing distribution would in this
case be simply the prior distribution mentioned above. Applying {\em Laplace smoothing}
we get:
\begin{equation}
\widehat{P}_w=\frac{\alpha \tilde{P}+ \mu_w \tilde{P}_w}{\alpha+\mu_w}
\end{equation}
where $\mu_w$ is the frequency of word $w$ in D, and $P_w$ is the
topic assignment distribution obtained from the word-topic matrix,
while $\alpha$ is the parameter that specifies the strength of the
prior.\\
{\bf (5) Conditioning on context words:} we propose a final enhancement to the word-topic
distributions. Suppose, the set of words $W=\{w_1,...,w_k\}$
represents a text snippet that we want to analyze. The word $w_i$ has
a specific meaning inside of $W$, that can be significantly different
than its meaning out of context. Denote
$W_{\bar{i}}=W-\{w_i\}$ as the set of all words in $W$ except
$w_i$. By $P_{W_{\bar{i}}}$, we denote the mixture distribution for $W_{\bar{i}}$ (Definition \ref{mixture}) and we use $\tilde{P}_{W_{\bar{i}}}$ when it is smoothed using Laplace smoothing method. We
propose the following definition of context-dependent word-topic
distribution:
\bed
Let $\tilde{P},\widehat{P}_{w_i}, \tilde{P}_{W_{\bar{i}}}$ be the topic prior, general
topic distribution for $w_i$, and the context distribution,
respectively. Then, the context-dependent distribution is
\begin{equation*}
P^{W_{\bar{i}}}_{w_i}(t)\propto \frac{\tilde{P}_{W_{\bar{i}}}(t)}{\tilde{P}(t)}\widehat{P}_{w_i}(t)
\end{equation*}
\eed
There is a probabilistic explanation that we have left out for 
lack of space. However this can be intuitively understood as follows:
we can think of $\frac{\tilde{P}_{W_{\bar{i}}}(t)}{\tilde{P}(t)}$ as a weight
that further reshapes the smoothed word-to-topic distribution $\widehat{P}_{w_i}(t)$
to take into account the context. In our experiments we also smooth this distribution
using {\em Laplacian} smoothing.

\section{Topic Similarities}
\label{sec:topic-similarities}
Using a topic distribution for describing an entity (e.g., a word, as in
the above section) has a certain
shortcoming. Namely, such a representation seems to imply that the
topics - used as atomic events in the distribution - are independent
from each other. In practice, that assumption is far from true. In
fact, even for the best topic model one can find a pair of topics that
are closely related. Those interrelations between topics are
properties that represent useful information about the model, and yet
may be completely ignored in a distributional representation of
topics. For example, if a word $w$ has topic distribution $P_w$
concentrated on some topic $t$, then it will 
not peak at all other topics that are very similar to
topic $t$. In this case, topic correlations would not be reflected in
the distribution. 

If we look at a topic distribution as a vector of coefficients $p$, then
we could think of the topics as representing a basis $t_1,...,t_T$ in
some hypothetical vector space $\cH$. In 
this case, $p$ corresponds to a weighted combination of the basis
vectors $\Sigma p_i t_i$, in other words, a point on the convex hull of the basis.
Suppose that $\cH$ is a Hiblert space (i.e. has a scalar
product). Now, the correlation between two topics can be represented
by the dot product of the corresponding basis vectors (we assume those
are unit vectors). We can consider a following alternative representation
of a point on the convex hull.

\bed
Let $p\in\rr^T$ be a vector. Denote $x=\sum_{i=1}^T p_i
t_i\in\cH$. Let $\widehat{p}\in\rr^T$ be defined so that
\[\widehat{p}_i = \langle x,t_i\rangle.\]
We will call $\widehat{p}$ the dot-product representation of $p$.
\eed

Notice, that if the considered topic basis is orthonormal, then both
representations are identical. Consider the Gram matrix
$\Sbb=\{\langle t_i,t_j\rangle\}_{i,j}$ of all possible dot products between
basis vectors. If the basis is orthonormal, this is simply the identity
matrix. Moreover, no matter which basis we choose, matrix $\Sbb$ is the
only information we need to translate $p$ to $\widehat{p}$.

\ber
For any distributional vector $p\in\rr^T$ (coefficients
sum to $1$), any Hilbert space $\cH$ and a set of
vectors $t_1,...,t_T\in\cH$, we have 
\[\widehat{p} = p\Sbb.\]
\eer

The dot-product representation allows us to incorporate the topic
similarity information into a vector representation. Moreover, we do
not need to directly manipulate the space $\cH$ to obtain it, except
computing pairwise topic correlations $s_{ij}$. However,
$\widehat{p}$ is not a probability distribution. Not only is it not
properly normalized, it may contain negative coefficients. To
alleviate that, we can add an additional condition that all of the
dot-products $s_{ij}$ are non-negative. A simple example satisfying
this constraint is if $\cH=\rr^n$ and the basis vectors have only
positive coefficients. For instance, if the topics are
themselves represented by probability distributions, then each $s_{ij}$
is related to taking cosine similarity between the pair of
distributions. 
We can naturally introduce topic similarities into our text model. We
first define the topic similarity matrix $\Sbb$ 
where the $i^{th}$ row of $\Sbb$ gives the topic similarity vector for
the $i^{th}$ topic (non-negative, e.g., cosine similarity between
topics). We further assume that each row has been normalized 
and hence can be thought of as a topic similarity distribution. Using
$\Sbb$ we replace the probability distribution for word $w_i$ with $P'_i=P_i\Sbb^T$which essentially diffuses the initial distribution to
one where  all topics similar to some topic $t$ are well
represented. Similarly, we can use $\Sbb$ to reflect the topic
similarity in the context distribution as $P' = P\Sbb^T$. The
stochastic interpretation here is that we draw a topic $t$ from the
original distribution, then we draw a second topic $t'$ from the
topic similarity distribution corresponding to $t$. Finally, we look
at the distribution of $t'$ as a random variable. We can redo
all of the calculations from previous section with those updated
distributions. To illustrate the benefit of this modification for
measuring diversity, consider a simple case where we have a pair of words
$S=(w_1,w_2)$, with stochastic representations $P_1,P_2$, each having
singleton support on one topic. Clearly, if those topics match, we
have no diversity, and if they don't, we get the diversity
corresponding to the entropy of a uniform distribution over two
elements. Notice, that without introducing topic similarity this is
independent of which pair of topics we are looking at. It could, for
instance, be ``Mathematics'' and ``Physics'' (closely related), or
``Mathematics'' and ``Literature'' (further apart).  Applying the
topic similarity matrix allows for this distinction to be clear and properly
reflected in the results.

{\bf Is this the right interpretation in this context? Should we use a
  more probabilistic approach?

Discuss specifically, how it this integrated with the rest of the
model. }
% \subsection{Topic Diversity}
% \label{sec:topic-diversity}
% In order to apply this model to natural language we first need to build a distributional representation for words. One
% natural choice for measuring the topic diversity is to use {\sl word-to-topic} distribution. We need to address the following problems:\\
% {\bf (1) Building word to topic distribution}: We train an
% LDA~\cite{Blei:2003:LDA:944919.944937} to build a topic model given a
% document corpus $\cC$. From this LDA topic model we obtain the
% word-topic-count matrix $\cM$ where $\cM_{ij}$ is the number of
% assignments of $j$-th topic to word $w_i$ in the corpus. By normalizing the rows of matrix $\cM$ we will obtain
% a word-topic distribution, where the $i$-th row of the matrix gives the topic distribution for the word $w_i$ (note that word-to-topic distribution as described above is not explicitly defined as a part of the standard LDA model and our approach is one way to approximate it). We also use $T$ to denote the set of topics learned in the LDA model.\\
% {\bf (2) Obtaining a prior topic distribution:} this is
% required for computing the information diversity as described in Section~\ref{sec:information-diversity}. We obtain a prior
%  topic distribution $P$ by computing the proportions of overall topic
%  assignments. This corresponds to summing up matrix $\cM$ along its
%  rows and then normalizing the resulting vector.\\

% {\bf (3) Capturing topic similarity:} here we consider the problem first raised in~\cite{bache:2013}. When building a word-to-topic distribution model based on 
% the word-to-topic co-occurrence matrix, the relationship among topics (i.e, topic similarity ) may be lost. For example, if a word $w$
% has a topic distribution $P_w$ concentrated on some topic $t$, then it will
% not necessarily peak at all other topics that are very similar to topic $t$. To address this problem, we first define a topic similarity matrix $\cS_{|T|\times|T|}$
% where the $i^{th}$ row of $\cS$ gives the topic similarity vector for
% the $i^{th}$ topic (e.g., cosine similarity between
% topics~\cite{bache:2013}). We further assume that each row is normalized 
% and hence can be thought of as a topic similarity distribution. Using $\cS$ we obtain $\tilde{P}_w = P_w\cS^T$ which essentially diffuses the initial distribution to
% one where  all topics similar to some topic $t$ are well represented. Similarly, we can use $\cS$ to reflect the topic similarity in the prior distribution as $\tilde{P} = P\cS^T$.
% In Section~\ref{sec:experiments} we will show how this
% approach enhances the standard entropic measure of diversity.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
\label{sec:datasets}
{\bf Write a more in depth discussion of the datasets and the
  experiments. }
We used the following two datasets in our experiments: (1) Interesting iPhone cases:
for generating the ground truth data we hired workers from {\em Amazon Mechanical Turk (AMT)} to label a collection
of nearly 20,000 iPhone cases on {\em eBay}. The details of this step is beyond the scope of this paper, however we used insights from
interesting iPhone cases found on {\em Pinterest} and {\em eBay's} user behavior data in order to generate a balanced data-set. 
We then pulled our final dataset from the annotated by selecting only those instances where the annotators all labeled it as
positive (i.e., interesting) or negative (i.e., uninteresting). The final data-set consists of 2179 positive and 9770 negative instances for
a total of 11,949 instances. For each instance, the product title of
the corresponding {\em eBay} listing was used as the input. In this case we are
dealing with very short text snippets, usually 10 to 12 words each. To
train a topic model, we used a larger, more broader set of about
2 million product titles, grouped based on {\em eBay} categorical information into about 8,000
documents of approximately 200 titles each; (2) {\em NSF}
abstracts: for the second dataset we used a set of 61,902 National Science Foundation
Scholarship proposal abstracts (see~\cite{bache:2013} for more details) to evaluate how our diversity measure
compares to other methods on larger pieces of text. We used this set
for training a topic model, however to get labeled data, we had to
generate artificial examples, by randomly mixing pairs of abstracts that we
could expect to be either similar (small diversity) or very different
(high diversity) and labeling them accordingly. We generated 5,000 of
those examples with positive and negative labels evenly represented. For both datasets, we used the Mallet LDA implementation and learned a separate topic model with $400$ topics.

\subsection{Baselines}
\label{sec:baselines}
{\bf More information about the baselines. Particularly, the entropy
  with topic similarity is essentially also a new approach and works
  well. 

Add the DPP baseline (expected sample size, because detereminant is
unstable). Discussion probably
should include the fact that it is correlated with the document
length, which has more impact with eBay dataset. Include plots of ROC
curves with them.

More info about the auto-encoders and word2vec method.}

We present two sets of results. First, we present
ROC curves comparing different entropic measures of topic diversity in an unsupervised setting 
(labeled data is only used for generating the curves). Figures~\ref{fig:phonecases-comparison} and \ref{fig:nsf-comparison}
compare our diversity metric using both topic similarity and context
conditioning (labeled by {\em JSD-Sim-Con}) with a few baselines;
namely LDA topic entropy, LDA topic entropy using topic similarity
(labeled by {\em Entropy-Sim}), {\em Rao diversity}
(see~\cite{bache:2013} for details). 

\subsection{Results}
\label{sec:results}
In either case it can be observed
that our diversity metric outperforms the other baselines with an AUC
around $0.73$. Moreover, for the {\em eBay} dataset the other measures
give poor results. This can be explained as follows: since the
text snippets are short, the LDA may yield a poor topic inference for such short text and as a result all measures using topic inference would perform poorly. Figures~\ref{fig:phonecases-breakdown} and \ref{fig:nsf-breakdown} show the gains we obtain by applying topic similarity and context conditioning techniques (steps 4 and 5) that we discussed in Section~\ref{sec:topic-diversity}. However, their degree of effects is different for each
dataset.  In the second set of results, we used the unnormalized vector of mixture topic
distribution (described in Definition~\ref{mixture}) computed over
{\em eBay} product titles in a supervised classification
setting. Table~\ref{tab:classification-results} shows
the performance of the SVM classifier using our proposed mixture topic
distribution as features and compares it to two different baselines, namely, SVM using
{\em Latent Semantic Indexing (LSI)} features (by forming a
document-term matrix and performing SVD), and a deep learning approach
using the {\em recursive auto-encoders (RAE)} framework described
in~\cite{Socher:2011:SRA:2145432.2145450}. These results are averaged
over five different cross-validation splits using $0.6$ for training
and $0.4$ for testing. Our proposed approach shows a higher precision
and a marginally higher accuracy compared to the baselines.

\section{Conclusions}
\label{sec:conclusions}
Our proposed approach shows a higher precision
and a marginally higher accuracy compared to the baselines.



\begin{table*}[t]
\label{tab:classification-results}
\vspace{-4mm}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
&Precision & Recall & F1 & Accuracy
\\ \hline 
JSD Features         &$\mathbf{0.714}\pm 0.015$&$0.597\pm 0.016$&$0.650\pm
0.014$& $\mathbf{0.8828}\pm 0.0045$\\
RAE             &$0.676\pm 0.005$&$\mathbf{0.666}\pm 0.030$&$\mathbf{0.671}\pm
0.013$&$0.8809\pm 0.0020$ \\
SVD Features             &$0.676\pm 0.008$&$0.633\pm 0.017$&$0.654\pm
0.010$&$0.8778\pm 0.0027$\\
\hline
\end{tabular}
\caption{Classification results for the eBay dataset.}
\end{center}
\end{table*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{icml2015stylefiles/icml2015}
\bibliography{icml2015}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}





