\subsection{Topic Similarities}
Using a topic distribution for describing an entity has a certain
shortcoming. Namely, such a representation seems to imply that the
topics - used as atomic events in the distribution - are independent
from each other. In practice, that assumption is far from true. In
fact, even for the best topic model one can find a pair of topics that
are closely related. Those interrelations between topics are
properties that represent useful information about the model, and yet
may be completely ignored in a distributional representation of
topics. For example, if a word $w$ has topic distribution $P_w$
concentrated on some topic $t$, then it will 
not peak at all other topics that are very similar to
topic $t$. In this case, topic correlations would not be reflected in
the distribution. 

If we look at a topic distribution as a vector of coefficients $p$, then
the topics represent a basis $t_1,...,t_T$ in some hypothetical vector space $\cH$. In
this case, $p$ corresponds to a weighted combination of the basis
vectors $\Sigma p_i t_i$, in other words, a point on the convex hull of the basis.
Suppose that $\cH$ is a Hiblert space (i.e. has a scalar
product). Now, the correlation between two topics can be represented
by the dot product of the corresponding basis vectors (we assume those
are unit vectors).We propose a following alternative representation
of a point on the convex hull.

\bed
Let $p\in\rr^T$ be a distributional vector. Denote $x=\sum p_i
t_i\in\cH$. Let $\widehat{p}\in\rr^T$ be defined so that
\[\widehat{p}_i = \langle x,t_i\rangle.\]
We will call $\widehat{p}$ the dot-product representation of $p$.
\eed

Notice, that if the considered topic basis is orthonormal, then both
representations are identical. Consider the Gram matrix
$\cS_{ij}=\langle t_i,t_j\rangle$ of all possible dot products between
basis vectors. If the basis is orthonormal, this is simply the identity
matrix. Moreover, no matter which basis we choose, matrix $\cS$ is the
only information we need to translate $p$ to $\widehat{p}$.

\bep
Observe that for any distributional vector $p\in\rr^T$, any Hilbert space $\cH$ and a set of
vectors $t_1,...,t_T\in\cH$, we have 
\[\widehat{p} = p\cS.\]
\eep

The dot-product representation allows us to incorporate the topic
similarity information into a vector representation. Moreover, we do
not need to directly manipulate the space $\cH$ to obtain it, except
computing pairwise topic correlations $\cS_{ij}$. However,
$\widehat{p}$ is not a distributional vector. Not only is it not
properly normalized, it may contain negative coefficients. To
alleviate that, we can add an additional condition that all of the
dot-products $S_{ij}$ are non-negative. A simple example satisfying
this constraint is if $\cH=\rr^n$ and the basis vectors have only
positive coefficients. For instance, if the topics are
themselves represented by distributions, then each $\cS_{ij}$
corresponds to the cosine similarity between the pair of
distributions.

\subsection{Word2Vec Topics}

Word2Vec is another form of word representations, which uses Euclidean
vectors that are not probability distributions. We ask whether
it is possible to derive a topic model from a set of such
representations. Suppose, for instance, that we want to find $k$
topics that are most representative of the provided vocabulary
represented as word vectors in $\rr^d$. It is logical to assume that
each topic be also represented as a vector in $\rr^d$. A question
arises, how do we represent each word as a topic distribution in this
scenario. Recall, that the central probability function modeled by the
word-embeddings is defined as follows. Given a set of words $w\in\cW$
and a set of contexts $c\in\cC$ (the sets can be identical), we look
for embeddings $v_c,v_w\in\rr^d$ for the words and the contexts
that best model the conditional probability  
\[p(c|w) = \frac{\e^{v_c\cdot v_w}}{\sum_{c'\in\cC}\e^{v_c'\cdot v_w}}.\]
Take a hypothetical set of topic vectors $v_{t_1},...,v_{t_k}\in\rr^d$. We can
describe them in the above model by their context probability
distributions $p(\cdot|t_i)$. We can now use a simple generative topic
model to describe this setting. Namely, suppose we are given a global
topic distribution $p(t)$. Next, imagine we generate a large amount of
text through the following process:
\begin{enumerate}
\item Draw a topic $t$ from the topic distribution $p(\cdot)$.
\item Draw a context $c$ from the distribution $p(\cdot|t)$.
\end{enumerate}

We can now use the Bayes rule to describe the topic distributions for
each context $c$:

\[p(t_i|c)\propto p(t_i)\cdot p(c|t_i) = p(t_i)\cdot
\frac{\e^{v_c\cdot v_{t_i}}}{Z_{v_{t_i}}},\]

where $Z_{v_{t_i}}=\sum_{c'\in\cC}\e^{v_c'\cdot v_{t_i}}$ is the normalization factor
from the previous equation. Now, if we select the global topic
distribution to be $p(t_i)\propto Z_i$, then this simplifies to:

\[p(t_i|c)=\frac{\e^{v_c\cdot v_{t_i}}}{\sum_{j=1}^k\e^{v_c\cdot
    v_{t_j}}}.\]

Having obtained the set of topic distributions for each $c\in\cC$, as
well as a prior topic distribution $p(t)$, we ask what is the best
choice of topic vectors $t_i$, so that those distributions bring the
most information about the vocabulary. How do we measure that
information? For instance, how much information is contained in a
single distribution $p(\cdot|c)$, for a fixed $c$? We
can use the KL-Divergence to compare it to the prior $p(\cdot)$. In
other words, we compute $D_{KL}(p(\cdot|c)\|p(\cdot))$. Now, we want to
combine this into a single measure for the set of all $c\in\cC$. For
this, we can use the Jensen-Shannon Diversity:
\[D_{JS}(\Sigma d_c p(\cdot|c)) = \sum_{c\in\cC}d_c D_{KL}(p(\cdot|c)\|\Sigma d_c p(\cdot|c)),\]
which would simply be a weighted average of the KL divergences of
conditional probabilities from the prior, which is itself derived as a
mixture of the conditional probabilities. Naturally, we can
probabilistically derive the prior as such a mixture as follows:
$p(\cdot)=\sum_{c\in\cC}p(c) p(\cdot|c)$.
This suggests that we should use the marginal probability of context
$c$ as the corresponding weight $d_c$. We compute this as
\[p(c)=\frac{\sum_{j=1}^k\e^{v_c\cdot v_{t_j}}}{\sum_{j=1}^k Z_{t_j}}.\]

Through this analysis, we claim that a good set of topic vectors can
be derived by maximizing the mixture diversity $D_{JS}(\Sigma
p(c)p(\cdot|c))$. 

% Combining the intuitions from the above section with the
% probabilistic interpretation of Word2Vec, we arrive at the following
% definition. 

% \bed
% Let $t_1,...,t_k\in\rr^n$ be the topic vectors. Let $w\in\rr^n$ be a
% word vector. Then, a {\bf topic distribution} $P_w$ of $w$ is defined as
% \[ P_w(i)\propto \exp(\langle w,t_i\rangle).\]
% \eed

% TODO: We need to find out whether this definition can be somehow
% properly derived from the Word2Vec probabilistic model.