\documentclass{article} % For LaTeX2e
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\usepackage{natbib}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\input{../defs}

\newtheorem{theoreme}{Theorem} %[section]
\newtheorem{proposition}[theoreme]{Proposition}
\newtheorem{lemma}[theoreme]{Lemma}
\newtheorem{definition}[theoreme]{Definition}
\newtheorem{corollary}[theoreme]{Corollary}
\newtheorem{remark}[theoreme]{Remark}
\newtheorem{example}[theoreme]{Example}
\newtheorem{examples}[theoreme]{Examples}
%\newtheorem{conjecture}[theoreme]{Conjecture}
\newtheorem{conjecture}{Conjecture}


\title{An Information Theoretic Approach to Quantifying Text Interestingness}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}

We study the problem of automatic prediction of text interestingness and present an information theoretic approach for
quantifying text interestingness in terms of text topic diversity. Our hypothesis is, in many text domains, often an interesting
concept is generated by mixing a diverse set of topics. Given a word distributional model learned from text corpus, we present
an algorithm that leverages {\sl Jensen-Shannon} divergence for measuring text diversity and demonstrate how such a measure
correlates with text interestingness. We describe several different base-line algorithms and present results over two different
data sets: a collection of e-commerce products from {\sl eBay} , and a collection of {\sl NSF} Awards from $2007$ to $2012$.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

With the rapid growth of E-Commerce, new products are increasingly populated into the market place on daily basis.  A larger subset of these products are 
categorized as daily needs or off-the-shelf products, while a much smaller subset can be attributed as {\em unique}, {\em creative}, {\em serendipitous }, {\em conversation starter}, or {\em interesting}. This class of products often provoke an emotive response in users and create a more engaging experience for the them (see {\sl pinterest}\footnote{\url{www.pinterest.com}} for example).  This is further illustrated in Figure~\ref{fig:ebay-products} where it shows a collection of {\sl eBay} products selected from several gadget related categories. The top row (Figures~\ref{fig:standard-iphone-case}, \ref{fig:standard-speaker}, and \ref{fig:standard-speaker}) shows examples of
off-the-shelf products that one might often see at stores, online sites, etc. In contast, the bottom row
(Figures~\ref{fig:eyeshadow-iphone-case}, \ref{fig:zeppelin-speaker}, and \ref{fig:geeky-clock}) shows examples of the products
from similar categories which are more unique and have a more creative design, and hence more {\em interesting}. 
\begin{figure}
        \centering
        \begin{subfigure}[b]{0.3\textwidth}
                \centering
                \includegraphics[width=25mm]{figures/standard-iphone-case.jpg}
                \caption{Black Qi Standard Wireless Charging Charger Receiver Case For iPhone 5 5G}
                \label{fig:standard-iphone-case}
        \end{subfigure}%
              ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\textwidth}
                \centering
                \includegraphics[width=25mm]{figures/standard-iphone-speaker.jpg}
                \caption{Bluetooth Wireless Speaker Mini Portable Super Bass For iPhone}
                \label{fig:standard-speaker}
        \end{subfigure}%
              ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\textwidth}
                \centering
                \includegraphics[width=25mm]{figures/standard-clock.jpg}
                \caption{Modern DIY Your Wall Clock Sticker Decoration Home Roman Numerals Silver SCY07}
                \label{fig:standard-speaker}
        \end{subfigure}\\
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\textwidth}
        	        \centering
                \includegraphics[width=30mm]{figures/eyeshadow-iphone-case.jpg}
                \caption{\textcolor{red}{{\bf Eyeshadow}} Palettes for \textcolor{red}{{\bf iPhone}} 6 case}
                \label{fig:eyeshadow-iphone-case}
        \end{subfigure}
              ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\textwidth}
		 \centering
                \includegraphics[width=30mm]{figures/horn-iphone-speaker.jpg}
\caption{White Silicone \textcolor{red}{{\bf Horn}} Stand Speaker for Apple \textcolor{red}{{\bf iPhone}} 4/ 4S}                \label{fig:zeppelin-speaker}
        \end{subfigure}
       ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.3\textwidth}
		 \centering
                \includegraphics[width=30mm]{figures/geeky-clock}
\caption{\textcolor{red}{{\bf Equation}} Wall \textcolor{red}{{\bf Clock}} Gifts for Math Gurus or Best Geek Friends}                \label{fig:geeky-clock}
        \end{subfigure}
       \caption{A collection of {\em eBay} products: top row shows some off-the-shelf products while the bottom row shows a more unique/inteesting products of similar categories}\label{fig:ebay-products}
\end{figure}


Quantifying interestingness is a challenging problem. First, the general notion of interestingness is not well defined.  Second, it is 
subjective; something that is interesting to someone may not be interesting to someone else. There has been considerable research on visual interestingness and aesthetic quality of images~\cite{Datta:2006:SAP:2129560.2129588,Ke:2006:DHF:1153170.1153495,IsolaParikhTorralbaOliva2011,dhar:2011,reinecke2013predicting,journals/pami/WeinshallZHKOABGNPHP12}. 
In text domain, researchers have studied some dimensions of this problem in terms of {\em humor identification}~\cite{Mihalcea:2005:MCL:1220575.1220642,Davidov:2010:SRS:1870568.1870582,Kiddon11,labutov-lipson:2012:ACL2012short}, {\em text aesthetics}~\cite{journals:tamd:Schmidhuber10,N13-1118,ganguly:2014}, and {\em document diversity}~\cite{bache:2013}. 

 In this paper we only focus on the text information. Our hypothesis is, many interesting products often present diversity in the text describing them. In examples \ref{fig:eyeshadow-iphone-case},
\ref{fig:zeppelin-speaker}, and \ref{fig:geeky-clock} we have highlighted words that offer the largest diversity in each case.
Fro example, in at the context of iPhone cases, one would expect less to observe topics that relate to make up and so forth
(Figure~\ref{fig:eyeshadow-iphone-case}). In this work we introduce an information-theoretic approach for measuring topic diversity based on {\em Jensen-Shannon Information Diversity} over a set of topics learned from a document corpus using {\sl Latent Dirichlet Allocation} (LDA)~\cite{Blei:2003:LDA:944919.944937}. The idea of quantigfying topic diversity has been previously studied by~\cite{bache:2013} for measuring document diveristy in a  collection of {\sl NSF} Awards documnts and also has been addressed in \cite{ganguly:2014} as  feature for measuring text asesthetic. We show how our method differs from these approaches and present empirical results over two different data sets: a collection of e-commerce products from {\sl eBay} , and a collection of {\sl NSF} Awards from $2007$ to $2012$. 


%% When looking at interestingness \cite{silva2006}















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Our Approach}
\label{sec:our-approach}

\subsection{Word Representation}
\label{sec:word-representation}

We assume a distributional representation for the words in the vocabulary (for a brief review
see~\cite{Turian10wordrepresentations}). A distributional representation over a vocabulary $W$ can be expressed in terms of a
co-occurrence matrix $A_{|W|\times|C|}$ where each row represents a co-occurrence of a word with a set of contexts deonted by
$C$. For example if we chose $C=W$ we get the familiar {\sl word-to-word} co-occurrence representation which counts the number
of times two words co-occurred in a document corpus. Another choice would be to use the set of topics learned over a document
corpus by {\sl Latent Dirichlet Allocation} (LDA)~\cite{Blei:2003:LDA:944919.944937} as the context. In our model rows of $A$
are then normalized in order to obtain a probability distribution over the context. We will use the notation $P_w$ to represent
the probability distribution of a word $w$ over the context. We also use the notation $T=\{w_1,...,w_k\}$ for a text
snippet\footnote{by text snippet we refer to a moderately short block of tex. For example in e-commerce data it could be the
title of a product, or in Twitter it could be the tweets.} where $\{w_1,...,w_k\}$ are the words in th eorder they appear in the
text. Given a word distributional, $T_{\cP}=\{P_{w_1},...,P_{w_k}\}$ denotes the set of probability distributions over the words
in the text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Information Diversity}
\label{sec:information-diversity}

Given a distributional representation over a vocabulary $W$ and context $C$ with $P_w$ giving a probability distribution of a
word $w$ over the context $C$, we can measure information diversity for a given text snippet $T=\{w_1,...,w_k\}$ and its
distributional representation $T_{\cP}=\{P_{w_1},...,P_{w_k}\}$ as follows:

\bed
Given a distribution $P_w$, its {\sl importance} with respect to a
prior distribution $P$ is defined as $D_w = D_{KL}(P_w\|P)$ where $D_{KL}(\|)$ denotes the
{\sl Kullback-Leibler} Divergence.
\eed

\bed\label{mixture}
Given a set of distributions $T_{\cP}=\{P_{w_1},...,P_{w_k}\}$ and a
prior $P$, we
define a mixture distribution $P_T$ as $P_T=\sum_{i=1}^k d_{w_i} P_{w_i}$ where $d_{w_i}=\frac{D_{w_i}}{\sum D_{w_j}}$ are the normalized
importances.
\eed

Essentially, $P_T$ is the weighted average of the set $T_{\cP}$, where
the weights are chosen according to the importances. Next we
define the diversity measure:

\bed\label{diversity}
We define the Jensen-Shannon Information Diversity of a set of
distributions $\cP_T$ with respect to 
prior $P$ as $D_S=\sum_{i=1}^k d_{w_i}D_{KL}(P_{w_i}\|P_T)$
where $d_{w_i}$ and $P_T$ are as in the previous definition.
\eed
This definition is closely related to the 
{\em general Jensen-Shannon Divergence}, defined in~\cite{FugledeTopsoe}. Another
interesting theoretical property of Jensen-Shannon Information
Diversity is that it can be interpreted as a generalization of Shannon
entropy as a population diversity measure, however we will not go
into this here any further. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Topic Diversity}
\label{sec:topic-diversity}

In order to apply this model to natural language we first need to build a distributional representation for words. One
natural choice which is closely tied to measuring topic diersity is to use {\sl word-topic} distribution. We need to solve the following two
questions:
\begin{enumerate}
\item {\bf How to obtain a topic distribution for a given word}: We train an
LDA~\cite{Blei:2003:LDA:944919.944937} to build a topic model given a document corpus $\cC$. From this topic model we obtain the
word-topic matrix $\cM$ where $M_{ij}$ is the number of times $i$-th word was assigned the $j$-th topics\footnote{Note that word-topic distribution as described, is not directly defined in the standard {\em LDA} model and hence is only one possible approximation.}. By normalizing the rows of matrix $\cM$ we will obtain
a word-topic distribution, where the $i$-th row of the matrix gives the topic distribution for the word $w_i$.
\item {\bf How to obtain a prior topic distribution} (which is
required for computing the information diversity as described in Section~\ref{sec:information-diversity}): We obtain a prior
 topic distribution by computing the proportions of overall topic
 assignments. This corresponds to summing up matrix $M$ along its
 columns (rows????), and then normalizing the resulting vector.
\end{enumerate}

The proposed choice of topic distributions for words has some
shortcomings. First, consider a word $w$ which occurs very rarely
in the entire corpus $\cC$. The topic distribution associated with this word
will be very concentrated (in other words the topic distribution associated with this word as the result of normzlizing the corresponding row in $\M$ is heavily biased). 
If  for this corpus it hapens that the prior topic distribution is close to uniform, that will give a very high importance to $w$, just because
there is not enough datapoints in our set to determine its {\em true}
topic distribution. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Observer's Model}
\label{sec:observer-model}

We will now present a model of a human Observer $A$ that is being presented with
the set $\cT$ and is supposed to judge the {\em interestingness} of
each piece of text from it. Additionally, we assume that $A$ has
gained their linguistic knowledge by observing samples from the set
$\cD$. What is, then, the general meaning (topic distribution) of a word $w$,
from the Observer's viewpoint? A statistically adequate approach is to
take a Bayesian model with a Dirichlet prior. We observe occurrences
in set $\cD$ of topic assignments for $w$ to learn the posterior
distribution. However, instead of using the fixed assignments produced
by the topic model, we let the Observer choose their own assignment as
follows: given a pair $(w,t)$, where $t$ is the topic assignment given
by the model for a specific occurrence of $w$, the observer selects a
topic from some topic-similarity distribution $S_t$, conditional on
$t$. An appropriate Dirichlet prior can be derived from this by
looking at the overall topic assignments (not for a specific
word). What would we get if we fed those to the Observer, letting $A$
generate their own topic for each?  Denoting by $S$ the matrix with
rows of topic-similarity distributions and by $P$ the (row vector) topic
distribution coming from the word-topic matrix, we find that the
observer's prior will concentrate around the distribution described by
the product $\widehat{P}=PS^T$. Using Bayes's rule to calculate the Observer's
posterior probability, we get
\[\widehat{P}_w=\frac{\alpha PS^T + \mu_w P_wS^T}{\alpha+\mu_w},\]
where $\mu_w$ is the frequency of word $w$ in D, and $P_w$ is the
topic assignment distribution obtained from the word-topic matrix,
while $\alpha$ is the parameter that specifies the strength of the
prior.

Next, let us analyze the Observer's behavior when reading a text
segment from $\cT$. We treat each piece of text as a bag of words,
disregarding the order. Suppose, the set of words is $T=\{w_1,...,w_k\}$. We
have already established how the Observer understands each word
separately. However, given a set of words, each one exists in the
context of the others. We can describe that context using the mixture
distribution from Definition \ref{mixture}. Denote
$T_1=\{w_2,...,w_k\}$ as the set of all words in $T$ except
$w_1$. What is the appropriate topic distribution for $w_1$, given a
context mixture distribution $P_{T_1}$? For this, we can look more
closely at the LDA model we used to obtain the word-topic matrix. We
can think of it as being generated by the following process: first
drawing a topic form the prior topic distribution, then drawing a word
from that topic's word distribution. It is natural to ask what would
the matrix look like if we used $P_{T_1}$ as the topic distribution
instead of the prior, and what would be the corresponding topic
distribution for word $w_1$. 

\bep
Let $\widehat{P},\widehat{P}_{w_1},P_{T_1}$ be the topic prior, general
topic distribution for $w_1$, and the context distribution,
respectively. Then, the context dependent distribution defined as
above, will be
\[P^{T_1}_{w_1}(t)\propto \frac{\widehat{P}_{w_1}\!(t)P_{T_1}\!(t)}{\widehat{P}(t)}.\]
\eep

The danger with relying on $P^{T_1}_{w_1}$
is that if the distributions $\widehat{P}_{w_1}$ and $P_{T_1}$ are
mostly disjoint, than their product will be very small. In other
words, we would need a long sampling process in generating the
hypothetical word-topic count matrix to obtain a statistically
significant estimation of the $P^{T_1}_{w_1}$. Once again,
we turn to bayesian analysis: we let the Observer use
$\widehat{P}_{w_1}$ as their maximum likelihood estimator in a
Dirichlet prior, obtaining the following posterior solution:
\[\widehat{P}^{T_1}_{w_1}(t)\propto \beta \widehat{P}_{w_1}\!(t) + \frac{\widehat{P}_{w_1}\!(t)P_{T_1}\!(t)}{\widehat{P}(t)}.\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiments}

\subsection{Data Sets}
\label{sec:data-sets}
\begin{itemize}
\item {\bf iPhone cases:}
\item{\bf NSF:}
\end{itemize}

\subsection{Baselines}
\label{sec:baselines}

Diversity:
\begin{itemize}
\item {\bf Shannon  Entropy:}
\item{\bf Topic Diversity:} \cite{ganguly:2014}, \cite{bache:2013}
\item{\bf Word frequency as distribution:}
\end{itemize}



Classification:
\begin{itemize}
\item {\bf Bag of words (BOW):}
\item{\bf Latent Semantic Indexing (LSI):}
\item{\bf Recursive Auto Encoders (RAE):}
\end{itemize}

\subsection{Results}
\label{sec:results}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{sec:conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}



\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\bibliography{nips2014}


\end{document}
